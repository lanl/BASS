
For our next example, we will test the package on the Friedman function \citep{friedman1991multivariate}.  This function will have 10 inputs, five of which contribute nothing.  The other five are used to generate
\begin{align}
f(\bx) &= 10\sin(\pi x_1x_2) + 20(x_3-0.5)^2 + 10x_4 + 5x_5.
\end{align}
We generate 200 input samples uniformly from a unit hypercube, calculate $f(x)$ for each and add standard Normal error to obtain data to model.
<<c2-1, cache=T>>=
set.seed(0)
f <- function(x) {
  10 * sin(pi * x[, 1] * x[, 2]) + 20 * (x[, 3] - 0.5)^2 +
    10 * x[, 4] + 5 * x[, 5]
}
sigma <- 1
n.vars <- 10
n <- 200
x <- matrix(runif(n * n.vars), n, n.vars)
y <- rnorm(n, f(x), sigma)
@

Here we will show how we can change the length of the MCMC chain and use parallel tempering.  We run the RJMCMC chain for 40000 iterations, discarding the first 30000 as burn-in and thinning by keeping every tenth sample.  We supply a temperature ladder with smallest value one (the ``cold chain'', or true posterior) and largest value $11.03$ (the ``hottest'' chain) using geometric spacing.  Thus, $t_i=(1+\Delta_t)^{i-1}$ where $\Delta_t$ is a spacing parameter we set at $0.35$.  We use nine chains.  By default, chains at neighboring temperatures will be allowed to swap after the first 1000 iterations.
<<c2-2, cache=T, dependson='c2-1'>>=
mod <- bass(x, y, nmcmc = 40000, nburn = 30000, thin = 10,
            temp.ladder = (1 + 0.35)^(1:9 - 1), verbose = FALSE)
@

We can generate posterior predictive samples just as we did in the curve fitting example.
<<c2-3, cache=T, dependson=c('c2-1','c2-2')>>=
n.test <- 1000
x.test <- matrix(runif(n.test * n.vars), n.test)
pred <- predict(mod, x.test, verbose = TRUE)
@
Plotting these samples against the true values of $f(x)$ shows that we have a good fit (Figure~\ref{fig:ex2plot1}).
<<ex2plot1, fig.cap='BASS prediction on test data -- Friedman function.',fig.width=6*.7,fig.height=6*.7>>=
fx.test <- f(x.test)
plot(fx.test, colMeans(pred))
abline(a = 0, b = 1, col = 2)
@

Now that we are considering a function of many variables, we may be interested in sensitivity analysis.  To get the Sobol' decomposition for each posterior sample, we use the \code{sobol} function.
<<c2-4, cache=T, dependson='c2-2', results='hide'>>=
sens <- sobol(mod, verbose = FALSE)
@
Note that when \code{verbose = TRUE}, this function prints after every 10 models (as with the \code{predict} function, vectorizing around models rather than MCMC iterations saves a large amount of time).  Depending on the number of basis functions and the number of models, this function can take significant amounts of time.  If that is the case, using a smaller set of MCMC iterations by specifying \code{mcmc.use} may be useful.

The default plotting for this kind of object (Figure~\ref{fig:ex2plot2}) shows boxplots of variance explained for each main effect and interaction that shows up in the BASS model.  It also shows boxplots of the total sensitivity indices.
<<ex2plot2, fig.height=6, fig.width=12, out.width='.8\\linewidth', fig.cap='BASS sensitivity analysis -- Friedman function.'>>=
plot(sens, cex.axis = 0.5)
@
If there are a large number of main effects or interactions that explain very small percentages of variation, we can show only the effects that are most significant.  For instance, we could show only the effects that, on average, explain at least 1\% of the variance (Figure~\ref{fig:ex2plot3}).
<<ex2plot3, fig.cap='Most important main effects and interactions -- Friedman function.'>>=
boxplot(sens$S[, colMeans(sens$S) > 0.01], las = 2,
        ylab = "proportion variance", range = 0)
@
As expected, we see that almost all of the variance is from the first five variables and the only strong interaction is between the first two variables.

As a final note for this example, we discuss tempering diagnostics.  We would like for neighboring chains to have swap acceptance rate of somewhere around 23\%.  Running \code{bass} with \code{verbose = TRUE} prints these acceptance rates every 1000 iterations.  At the completion of the sampling, we can investigate acceptance rates by dividing the swap counts by the number of swap proposals, as follows.
<<tidy=T>>=
mod$count.swap / mod$count.swap.prop
@
Since we have specified nine temperatures, there are eight possible swaps, hence the eight numbers.  If, for example, we wanted to increase the first acceptance rate, we would move the second temperature closer to the first.

Further analysis of swapping can be done by looking at swap trace plots.
<<ex2plot4, fig.height=5, fig.width=12, out.width='\\linewidth', fig.cap='Parallel tempering diagnostics -- swap trace plot.',dev='png'>>=
matplot(mod$temp.val, type = "l", ylab = "temperature index")
@
Figure~\ref{fig:ex2plot4} shows the swap trace plot where $y$-axis values are temperature indices (1 is the true posterior and 9 is the posterior raised to the smallest power), the $x$-axis shows MCMC iteration and the colored lines represent the different chains.  We want to see these chains mixing throughout.

Determining whether the smallest value of the temperature ladder is small enough to allow for good mixing can be difficult.  In this example, we could run the model with \code{temp.ladder = 11.03} and look at mixing diagnostics.  One could also look at predicted versus observed plots at the different temperatures for the last MCMC iteration by executing the following code, the output of which is shown in Figure~\ref{fig:ex2plot5}.
<<ex2plot5, out.width='.7\\linewidth', out.extra='trim = 0 5 0 15, clip', fig.cap='Predicted versus observed for the last MCMC iteration of the nine chains at different temperatures.  The temperatures are shown above each plot.'>>=
par(mfrow=c(3,3))
temp.ind <- sapply(mod$curr.list, function(x) x$temp.ind)
for(i in 1:length(mod$temp.ladder)) {
  ind <- which(temp.ind == i)
  yhat <- mod$curr.list[[ind]]$des.basis %*% mod$curr.list[[ind]]$beta
  plot(yhat, y, main = round(mod$temp.ladder[i], 2))
  abline(a = 0, b = 1, col = 2)
}
@
Note that the \code{curr.list} object is a list with number of elements equal to the number of temperatures.  This list contains the MCMC state for each chain.  Since we swap temperatures rather than entire states, the chains are not in order according to temperature.
We note that using the default prior for $\sigma^2$ with a temperature ladder with relatively large values can lead to instabilities when estimating $\sigma^2$.  In cases where that is clearly the case, the prior for $\sigma^2$ will be automatically changed and a warning will be generated.

To demonstrate what is different when we use tempering, consider the equivalent BASS model fit without tempering.
<<c2-5, cache=T, dependson='c2-1'>>=
mod.noTemp <- bass(x, y, nmcmc = 40000, nburn = 30000,
                   thin = 10, verbose = FALSE)
@
We compare the root mean square prediction error (RMSE) for the two models, as well as the empirical coverage of 95\% probability intervals.  First, the RMSE for the model fit without tempering
<<c2-6, cache=T, dependson=c('c2-3','c2-5')>>=
pred.noTemp <- predict(mod.noTemp, x.test)
sqrt(mean((colMeans(pred.noTemp) - fx.test)^2))
@
and the empirical coverage
<<>>=
quants.noTemp <- apply(pred.noTemp, 2, quantile, probs = c(0.025, 0.975))
mean((quants.noTemp[1, ] < fx.test) & (quants.noTemp[2, ] > fx.test))
@
demonstrate that the fit is quite good.  When we use parallel tempering, the RMSE
<<>>=
sqrt(mean((colMeans(pred) - fx.test)^2))
@
and the empirical coverage
<<>>=
quants <- apply(pred, 2, quantile, probs = c(0.025, 0.975))
mean((quants[1, ] < fx.test) & (quants[2, ] > fx.test))
@
tend to be moderately better.  Under different seeds, we tend to see higher coverage when we use tempering and lower coverage when we do not.  We also tend to get better models in terms of RMSE when we use tempering.  Other benefits of tempering will be shown in later examples.  Because the computational burden is currently linear in the number of temperatures, using fewer temperatures is better.  Thus, for many purposes, the model without tempering may be good enough.



We also point out that this modeling framework can handle correlated inputs.  For instance, we use the correlation matrix
<<>>=
S <- matrix(.99, nrow=10, ncol=10) + diag(10) * 0.01
@
as a covariance matrix for simulated inputs, and rescale them to be between zero and one so that Friedman function evaluations are comparable to the ones we have used above.  The inputs are all highly correlated. The model fitting and prediction are done without any changes from what we have done previously.
<<c2-7, cache=T>>=
library(MASS)
x <- mvrnorm(n, rep(0, 10), S)
x <- apply(x, 2, BASS:::scale.range)
y <- rnorm(n, f(x), sigma)

mod <- bass(x, y, nmcmc = 40000, nburn = 30000, thin = 10,
            temp.ladder = (1 + 0.35)^(1:9 - 1), verbose = FALSE)

n.test <- 1000
x.test <- mvrnorm(n.test, rep(0,10), S)
x.test <- apply(x.test, 2, BASS:::scale.range)
pred <- predict(mod, x.test)
@
<<ex2plot6, echo=F, fig.cap='BASS prediction on test data -- Friedman function with correlated inputs.',fig.width=6*.7,fig.height=6*.7>>=
fx.test <- f(x.test)
plot(fx.test, colMeans(pred))
abline(a = 0, b = 1, col = 2)
@
We still get good prediction, as seen in Figure~\ref{fig:ex2plot6}.  We abstain from performing a sensitivity analysis under correlated input assumptions.  Note that if we want to assume independence between the inputs when we do a sensitivity analysis, we can use correlated training data.  However, we will likely be requiring the model to extrapolate outside the training data when we integrate over independent ranges, which can lead to poor results.
